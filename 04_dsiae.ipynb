{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9288f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "# Set environment variables for deterministic GPU operations\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Complete warning suppression\n",
    "warnings.simplefilter(\"ignore\")\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings too\n",
    "\n",
    "# Redirect stderr temporarily to suppress low-level warnings\n",
    "class SuppressOutput:\n",
    "    def __enter__(self):\n",
    "        self._original_stderr = sys.stderr\n",
    "        sys.stderr = open(os.devnull, 'w')\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stderr.close()\n",
    "        sys.stderr = self._original_stderr\n",
    "\n",
    "\n",
    "\n",
    "# GPU setup - do this BEFORE any other TensorFlow operations\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU memory growth enabled for {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU setup error: {e}\")\n",
    "\n",
    "import flopy\n",
    "import pyemu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import herebedragons as hbd\n",
    "import shutil\n",
    "from pyemu.emulators import DSIAE\n",
    "\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "\n",
    "# This should show your GPU\n",
    "print(\"Physical devices:\", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e322a71",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "In a previous notebook we ran good 'ole vanilla PCA based DSI. Here we introduce the new DSI-AutoEncoder class.\n",
    "\n",
    "In standard PCA-based data space inversion, the relationship between model parameters and observations is represented in a reduced-dimensional space defined by linear combinations of principal components. This linear reduction efficiently captures dominant variance but may miss nonlinear features of the system. In contrast, autoencoder-based data space inversion uses a neural network to learn a nonlinear mapping between the high-dimensional model outputs and a compact latent representation. This allows the autoencoder to capture more complex, nonlinear relationships in the data, potentially improving inversion performance in systems where responses are not well described by linear structures.\n",
    "\n",
    "### Autoencoders\n",
    "\n",
    "Autoencoders are neural networks designed to learn efficient, compressed representations of data. They consist of two parts: an **encoder**, which maps the input data into a lower-dimensional latent space, and a **decoder**, which reconstructs the original data from this latent representation. By training the network to minimize the difference between the input and reconstructed output, autoencoders learn to capture the most important, often nonlinear, features of the data.\n",
    "\n",
    "Autoencoders can leverage specialized architectures like **convolutional** and **LSTM** layers to better capture structure in the data. **Convolutional autoencoders** are well suited for spatially structured data, as they learn local spatial features and patterns. **LSTM-based autoencoders**, on the other hand, are designed for sequential or time-series data, capturing temporal dependencies and dynamics in the latent representation. By using these architectures, autoencoders can more effectively encode and reconstruct complex spatial or temporal relationships in the data.\n",
    "\n",
    "## what we are going to do\n",
    "\n",
    "In this noteook we are going to throught the mechanics of using the `pyemu.DSIAE` class and some of the caveats. This is a vanilla AE (think non-linear PCA), without any fancy LSTM or convolutional layers.\n",
    "\n",
    "THe first section mirrors the same steps as in the DSI notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e1536",
   "metadata": {},
   "source": [
    "# Getting ready\n",
    "## Load the Prior MC results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94073757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the temporary working folder\n",
    "org_md = os.path.join('master_prior_mc')\n",
    "pst = pyemu.Pst(os.path.join(org_md,'pest.pst'))\n",
    "oe = pst.ies.obsen.copy()\n",
    "oe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4472c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = flopy.mf6.MFSimulation.load(sim_ws='model', load_only=[],verbosity_level=0)\n",
    "gwf = sim.get_model('gwf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9c10a",
   "metadata": {},
   "source": [
    "## Choose the truth\n",
    "\n",
    "Lets choose one of the realizations as the rtuth, then remove if form the training data. We are going to select an inconvenient truth: one in which the max temperature plume extends a bit farther than most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53981534",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obsnmes = obs.loc[obs.oname==\"temp\"].obsnme.tolist()\n",
    "\n",
    "# find columns in data[obsnmes] where 50% of the values are below 16.01\n",
    "cols = oe[obsnmes].columns[(oe[obsnmes] <= 16.005).mean() > 0.5]\n",
    "print(len(cols), len(obsnmes))\n",
    "\n",
    "# find col in cols with highest std\n",
    "stds = oe[cols].std()\n",
    "cols = stds.sort_values(ascending=False).index.tolist()\n",
    "\n",
    "\n",
    "target_col = cols[0]\n",
    "print(target_col)\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(4,4))\n",
    "\n",
    "oe.loc[:,target_col].hist(ax=ax)\n",
    "\n",
    "truth_index = oe[target_col].sort_values().index.values[-15]\n",
    "ax.axvline(oe.loc[truth_index, target_col], color='r')\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27731288",
   "metadata": {},
   "source": [
    "Lets keep that for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e2bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = oe.loc[truth_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271cb5cb",
   "metadata": {},
   "source": [
    "Drop the truth form the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d485906",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = oe.loc[~oe.index.isin([truth_index]), :]\n",
    "assert data.shape[0] == oe.shape[0] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed7223",
   "metadata": {},
   "source": [
    "## Set observation values and weights\n",
    "\n",
    "Use the simulated values from the truth real as calibration targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fc667",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs.loc[truth.index,'obsval'] = truth.values\n",
    "obs.oname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f24a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.loc[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f55400",
   "metadata": {},
   "source": [
    "We have a little utility function in `herebdragons.py` to get the cell ids that correspond to \"measured\" heads (btw, these match obs locations in the original non-python tutorials):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_obs = hbd.get_obs_cellids(org_md)\n",
    "calib_obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f4261e",
   "metadata": {},
   "source": [
    "Set non-zero weights to these observations. We are history matching for the historical (past) flow stress period. \n",
    "\n",
    "We also have measured heads at specified locations, as well as a section of the river that is gauged. Lets just use an arbitrary assumption of stdv of 0.1 and 0.0001 for heads and riv rate, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad30f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "_obs = obs.loc[obs.oname=='heads0'].copy()\n",
    "_obs['i'] = _obs['i'].astype(int)\n",
    "_obs.sort_values(by=['i'],inplace=True)\n",
    "\n",
    "nzobsnmes = _obs.loc[_obs['i'].isin(calib_obs.icpl.values)].obsnme.tolist()\n",
    "assert len(nzobsnmes) >0\n",
    "\n",
    "obs.loc[nzobsnmes,'weight'] = 1.0 / 0.1\n",
    "obs.loc[nzobsnmes,'standard_deviation'] = 0.1\n",
    "\n",
    "obs.loc[obs.oname=='riv', 'weight'] = 1.0 / 0.0001\n",
    "obs.loc[obs.oname=='riv', 'standard_deviation'] = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c94a4d",
   "metadata": {},
   "source": [
    "Just for fun, lets look at the true K and hyperparameter fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6aa5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "onames =['k', 'npfkpp-aniso', 'npfkpp-bearing', 'npfkpp-corrlen']\n",
    "\n",
    "fig,axs = plt.subplots(1,4,figsize=(16,4))\n",
    "\n",
    "for e,oname in enumerate(onames):\n",
    "    ax = axs[e]\n",
    "    ax.set_aspect(\"equal\")\n",
    "    pm = flopy.plot.PlotMapView(model=gwf, ax=ax)\n",
    "\n",
    "    _obs = obs.loc[obs.oname==oname].copy()\n",
    "    _obs[\"i\"] = _obs[\"i\"].astype(int)\n",
    "    _obs.sort_values(\"i\", inplace=True)\n",
    "    obsnmes = _obs.obsnme.tolist()\n",
    "    arr = obs.loc[obsnmes].obsval.values\n",
    "    if oname=='k':\n",
    "        arr = np.log10(arr)\n",
    "        oname = \"log10(k)\"\n",
    "\n",
    "    pa = pm.plot_array(arr)\n",
    "    plt.colorbar(pa, ax=ax, shrink=0.5)\n",
    "\n",
    "    ax.set_title(oname)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "\n",
    "fig.tight_layout();\n",
    "plt.show()\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee80756",
   "metadata": {},
   "source": [
    "And the true max temperature field. This is our prediction of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc8067",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "\n",
    "oname = \"temp\"\n",
    "\n",
    "ax.set_aspect(\"equal\")\n",
    "pm = flopy.plot.PlotMapView(model=gwf, ax=ax)\n",
    "\n",
    "_obs = obs.loc[obs.oname==oname].copy()\n",
    "_obs[\"i\"] = _obs[\"i\"].astype(int)\n",
    "_obs.sort_values(\"i\", inplace=True)\n",
    "obsnmes = _obs.obsnme.tolist()\n",
    "arr = obs.loc[obsnmes].obsval.values\n",
    "#f oname=='k':\n",
    "#    arr = np.log10(arr)\n",
    "\n",
    "pa = pm.plot_array(arr)\n",
    "plt.colorbar(pa, ax=ax, shrink=0.5)\n",
    "\n",
    "ax.set_title(oname)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "\n",
    "fig.tight_layout();\n",
    "plt.show()\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e08b337",
   "metadata": {},
   "source": [
    "# Start DSI-AE\n",
    "\n",
    "For DSI, we dont need all the observations we have been tracking. We only really need the non-zero weighted obs and the prediction sof interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs.oname.unique()\n",
    "\n",
    "keep_obs = obs.loc[obs.weight > 0].obsnme.tolist()\n",
    "keep_obs.extend(obs.loc[obs.oname=='temp'].obsnme.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a3b6e",
   "metadata": {},
   "source": [
    "Lets use normal score transformation (usualy a good choice..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [\n",
    "            {\"type\":\"normal_score\"},\n",
    "            #{\"type\":\"standard_scaler\"}\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb31b3",
   "metadata": {},
   "source": [
    "And now we are good to build the DSI-AE surrogate. Note we use the exact same arguments as we did in the DSI notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsi = DSIAE(pst=pst, #optional...\n",
    "          data = data[keep_obs],\n",
    "          transforms=transforms,\n",
    "           energy_threshold=.9999, # the truncated-svd energy threshold\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93fa687",
   "metadata": {},
   "source": [
    "## Training the AE\n",
    "Here is where things start to differ.  For `DSI`, \"fitting\" the model just meant doing SVD. For `DSIAE` it means training the neural net to encode/decode the data set. There is a bit more user input and options for hyperparameter tunning available.\n",
    "\n",
    "### Machine learning dependency\n",
    "The `DSIAE` class uses `tensorflow` in the background. `Tensorflow` is an open-source machine learning framework developed by Google, widely used for building and training neural networks, including autoencoders. It provides flexible tools for both research and production, supporting CPUs, GPUs, and TPUs for scalable computation. While powerful, `tensorflow`s performance and installation can vary across operating systems—GPU acceleration, for instance, often requires careful setup of compatible CUDA and cuDNN libraries on Windows or Linux. Additionally, version compatibility between `tensorflow`, Python, and hardware drivers can sometimes pose challenges in maintaining stable environments.\n",
    "\n",
    "It is not our intention here to provide traning on machine learning (there are many btter resources out there to lear from...). We assume the reader has some knowledge on machine learning, and the use of tensorflow. Here are some basic insghts:\n",
    "\n",
    "* **validation_split=0.1** – Reserves 10% of the data for validation, helping monitor overfitting during training. Typical values range from 0.1 to 0.2; larger splits may be used when data are abundant.\n",
    "* **hidden_dims=(128, 64)** – Defines the number and size of hidden layers in the autoencoder. More layers or larger dimensions increase model capacity but may cause overfitting; smaller architectures are preferable for simpler data.\n",
    "* **lr=0.0001** – The learning rate controls how quickly the model updates its weights. Too high can cause instability; too low can slow convergence. Start with 1e-3 or 1e-4 and adjust based on training behaviour.\n",
    "* **epochs=300** – The number of complete passes through the dataset. More epochs allow better learning but risk overfitting; early stopping can mitigate this.\n",
    "* **batch_size=32** – The number of samples processed before updating model weights. Smaller batches improve generalization but train slower; 32–128 is common.\n",
    "* **early_stopping=True** – Stops training automatically if validation loss stops improving, preventing overfitting and saving time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b174fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsi.fit(\n",
    "        validation_split=0.1, # the fraction of data to use for validation during training\n",
    "        hidden_dims=(64,32),  # the architecture of the autoencoder\n",
    "        lr=0.001,  # learning rate for the optimizer\n",
    "        epochs=300,  # number of training epochs\n",
    "        batch_size=32,  # size of each training batch\n",
    "        early_stopping=True,  # whether to stop training early if validation loss doesn't improve\n",
    "        random_state=42,  # random seed for reproducibility\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30acb4d0",
   "metadata": {},
   "source": [
    "If you wish, pyemu has a built in hyperparameetr tuner (again, just using tensorflow in the background) that undertakes a grid serach to do hyperparameter tunning. Uncomment the code below if you wish to exeriment. It can take a few minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b17a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = dsi.hyperparam_search(\n",
    "#                latent_dims=None,\n",
    "#                latent_dim_mults = [0.75, 1.0, 1.25],\n",
    "#                hidden_dims_list=[(64,32),(128,64),(256,128)],\n",
    "#                lrs=[1e-3, 1e-4, 1e-5],\n",
    "#                epochs=300,\n",
    "#                batch_size=32,\n",
    "#                random_state=42\n",
    "#            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32e18e",
   "metadata": {},
   "source": [
    "## Encoding/Decoding\n",
    "\n",
    "A convenient aspect of DSI-AE versus vanilla DSI, is that we can directly project/encode the physics-based model outputs directly to latent space and back again. This allows us to estimate the \"error\" we incurr from simplification across data space. \n",
    "\n",
    "Here is how we do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb6c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = dsi.encode(dsi.data)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f7a8a6",
   "metadata": {},
   "source": [
    "`Z` is the ensemble of latent space parameters that directly correspond to the original \"prior\" ensemble of FullOrderModel (FOM) outputs. These are the parameters that `pestpp` sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c3bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9abcb6",
   "metadata": {},
   "source": [
    "We can project these back to data space by \"predciting\" (aka decoding) with the dsiae object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c78a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat = dsi.predict(Z)\n",
    "X_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e43e4",
   "metadata": {},
   "source": [
    "Let's see how well we did at reconstructing the `base` realization.\n",
    "\n",
    "Pretty good, with some error (as expected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994d9009",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data.loc['base',keep_obs],\n",
    "            X_hat.loc['base',keep_obs],\n",
    "            marker='.',\n",
    "            alpha=0.5)\n",
    "plt.xlabel('Original Data')\n",
    "plt.ylabel('Reconstructed Data')\n",
    "plt.title('Original vs Reconstructed Data Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87030b5c",
   "metadata": {},
   "source": [
    "# Setup for pestpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58917981",
   "metadata": {},
   "source": [
    "And setup the DSI pest dir...\n",
    "\n",
    "key detail now, spceify the `use_runstor=True` argument to setup the forward run using the external manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsi_t_d = \"pst_template_dsiae\"\n",
    "\n",
    "dpst = dsi.prepare_pestpp(t_d = dsi_t_d,\n",
    "                          use_runstor=True)\n",
    "dpst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5efdff",
   "metadata": {},
   "source": [
    "We can take a look at what the forward run function looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c64d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open forward_run.py and print\n",
    "with open(os.path.join(dsi_t_d, 'forward_run.py'),'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80c7b5e",
   "metadata": {},
   "source": [
    "## AE latent space prior\n",
    "\n",
    "In vanilla DSI, the latent space prior is straightforward: Guasiian normal distributions with mean 0 and stdv 1 for all latent space parameters.\n",
    "\n",
    "In DSI-AE things get a bit more complicated. Autoencoder-based approaches learn a nonlinear latent space, and the resulting latent parameter distribution is generally non-Gaussian and data-driven. This flexibility allows autoencoders to capture more complex, multimodal relationships in the data, though it can make interpretation and sampling from the latent space less straightforward than in PCA-based methods.\n",
    "\n",
    "Lets take a look at one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95325fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.iloc[:,0].hist()\n",
    "plt.xlabel('Latent Parameter')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Latent Parameter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357759c2",
   "metadata": {},
   "source": [
    "If we are happy using the same number of realizations with DSIAE as we had realizations in the training data set, then we are fine. We can explicilty pass the encoded FOM prior values as the dsiae-prior parameter ensemble to pestpp-ies.\n",
    "\n",
    "If we want to use a larger ensemble (usualy good ideia if we can aford it..), then we need to do a bit more work to empirically sample the dsi-ae latent parameter prior distribution...\n",
    "\n",
    "Lets do that now, using some tricks built into pyemu. First, load the latent space prior as a `ParameterEnsemble` object. This was pre-preared when you called `.prepare_pespp()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8891da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpst.pestpp_options['ies_parameter_ensemble']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d032d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_pe = pyemu.ParameterEnsemble.from_binary(dpst,\n",
    "                                                os.path.join(dsi_t_d, dpst.pestpp_options['ies_parameter_ensemble']))\n",
    "latent_pe.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a59165",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_pe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe13a9",
   "metadata": {},
   "source": [
    "We can now use the `draw_new_ensemble()` function to draw new realizations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1882f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nreals = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b830f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_ext = latent_pe.draw_new_ensemble(nreals-latent_pe.shape[0])\n",
    "#pe_ext.enforce()\n",
    "pe_ext.shape, latent_pe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87536c",
   "metadata": {},
   "source": [
    "Lets comapre the distirbution of one of the parameters to see what this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_pe.iloc[:,0].hist()\n",
    "pe_ext.iloc[:,0].hist(alpha=0.5)\n",
    "plt.xlabel('Latent Parameter')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Latent Parameter with Extended Ensemble')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d46ec",
   "metadata": {},
   "source": [
    "Great! Merge them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cef34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_ext = pd.concat([pe_ext,latent_pe],axis=0)\n",
    "# find index position of 'base' in latent_pe\n",
    "idx = np.where(pe_ext.index.values == 'base')[0][0]\n",
    "pe_ext.reset_index(drop=True,inplace=True)\n",
    "# name the row at idx 'base'\n",
    "pe_ext.rename(index={idx:'base'},inplace=True)\n",
    "\n",
    "assert (pe_ext.loc['base'] == latent_pe.loc['base']).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633da096",
   "metadata": {},
   "source": [
    "And write the extended parametr ensemble back to the pest dir, whislt also updateing the pespp options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1044fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.ParameterEnsemble(dpst,pe_ext).to_binary(os.path.join(dsi_t_d,'prior_extended.jcb'))\n",
    "dpst.pestpp_options['ies_parameter_ensemble'] = \"prior_extended.jcb\"\n",
    "pe_ext.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc6e1b0",
   "metadata": {},
   "source": [
    "And we are good to go...sheesh that was hard.\n",
    "\n",
    "Lets  run for 3 iterations. WThis is more that we diod for DSI. Why? Becasue the relationship is not as linear as for DSI, getting a good fit is more chalenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbc05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpst.control_data.noptmax = 3\n",
    "dpst.pestpp_options[\"ies_num_reals\"] = nreals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60caf16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpst.pestpp_options[\"ies_multimodal_alpha\"] = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpst.write(os.path.join(dsi_t_d, \"dsi.pst\"),version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b56b3",
   "metadata": {},
   "source": [
    "Get the executables again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hbd.get_bins(dsi_t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96042206",
   "metadata": {},
   "source": [
    "# Run pestpp with /e\n",
    "\n",
    "Right on! We are ready to get cracking. Let's run pestpp-ies and see what we get. \n",
    "\n",
    "Make a copy for safekeeping..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "md = f\"master_dsiae\"\n",
    "\n",
    "if os.path.exists(md):\n",
    "    shutil.rmtree(md)\n",
    "shutil.copytree(dsi_t_d, md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae57f5b",
   "metadata": {},
   "source": [
    "Now we will run `pestpp-ies` with the external run manager option. We do this by calling `pestpp-ies [controlfile].pst /e`. The `/e` trigegrs the external run manager option.\n",
    "\n",
    "Note that this run is in \"serial\" as far as `pestpp` is concerned. We are handling the paralelization of the dsi forward runs.\n",
    "\n",
    "Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb78f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run('pestpp-ies dsi.pst /e', cwd=md,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beed875",
   "metadata": {},
   "source": [
    "# Read pest results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(md,\"dsi.pst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af837509",
   "metadata": {},
   "source": [
    "See how it take slonger to get better fits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dbdd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.ies.phiactual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3fe25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs.oname.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e57b99",
   "metadata": {},
   "source": [
    "Get the posterior observation ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "obsen = pst.ies.obsen.copy()\n",
    "oe = obsen.loc[pst.control_data.noptmax]\n",
    "oe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78809d4d",
   "metadata": {},
   "source": [
    "See how well we fit the head obs data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b51cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nzobsnmes = pst.nnz_obs_names\n",
    "\n",
    "nzobsnmes = pst.nnz_obs_names[:-1]\n",
    "fig,axs = plt.subplots(1,2,figsize=(6,3))\n",
    "\n",
    "ax = axs[0]\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('head obs')\n",
    "\n",
    "[ax.scatter(obs.loc[nzobsnmes].obsval, oe.loc[i,nzobsnmes],c='b',marker='.') for i in oe.index];\n",
    "\n",
    "xmax = max(ax.get_xlim()[0],ax.get_ylim()[0])\n",
    "ymax = max(ax.get_xlim()[1],ax.get_ylim()[1])\n",
    "limax = max(xmax,ymax)\n",
    "xmin = min(ax.get_xlim()[0],ax.get_ylim()[0])\n",
    "ymin = min(ax.get_xlim()[1],ax.get_ylim()[1])\n",
    "limn = min(xmin,ymin)\n",
    "ax.plot([limn,limax],[limn,limax],'r--')\n",
    "ax.set_xlim(limn,limax)\n",
    "ax.set_ylim(limn,limax)\n",
    "\n",
    "\n",
    "nzobsnmes = pst.nnz_obs_names[-1:]\n",
    "ax = axs[1]\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title(nzobsnmes[0])\n",
    "\n",
    "[ax.scatter(obs.loc[nzobsnmes].obsval, oe.loc[i,nzobsnmes],c='b',marker='.') for i in oe.index];\n",
    "\n",
    "xmax = max(ax.get_xlim()[0],ax.get_ylim()[0])\n",
    "ymax = max(ax.get_xlim()[1],ax.get_ylim()[1])\n",
    "limax = max(xmax,ymax)\n",
    "xmin = min(ax.get_xlim()[0],ax.get_ylim()[0])\n",
    "ymin = min(ax.get_xlim()[1],ax.get_ylim()[1])\n",
    "limn = min(xmin,ymin)\n",
    "ax.plot([limn,limax],[limn,limax],'r--')\n",
    "ax.set_xlim(limn,limax)\n",
    "ax.set_ylim(limn,limax)\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497a97f",
   "metadata": {},
   "source": [
    "Make some plots of max temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b3b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "oname = \"temp\"\n",
    "\n",
    "for i in oe.index.values[-5:]:\n",
    "    fig,ax = plt.subplots(1,1,figsize=(6,6))\n",
    "\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    pm = flopy.plot.PlotMapView(model=gwf, ax=ax)\n",
    "    #pm.plot_grid(alpha=0.1,lw=0.1)\n",
    "\n",
    "    _obs = obs.loc[obs.oname==oname].copy()\n",
    "    _obs[\"i\"] = _obs[\"i\"].astype(int)\n",
    "    _obs.sort_values(\"i\", inplace=True)\n",
    "    obsnmes = _obs.obsnme.tolist()\n",
    "    diverg = max(oe.loc[:,obsnmes].values.max(), abs(oe.loc[:,obsnmes].values.min()))\n",
    "    vmax = np.ceil(diverg)\n",
    "    vmin = 16 \n",
    "    \n",
    "    arr = oe.loc[i,obsnmes].values\n",
    "    #arr = arr.reshape(61,gwf.modelgrid.ncpl).max(axis=0)\n",
    "    #f oname=='k':\n",
    "    #    arr = np.log10(arr)\n",
    "\n",
    "    pa = pm.plot_array(arr, cmap=\"Reds\", vmin=vmin,vmax=vmax)\n",
    "    plt.colorbar(pa, ax=ax, shrink=0.5)\n",
    "\n",
    "    ax.set_title(oname)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    fig.tight_layout();\n",
    "    plt.show()\n",
    "    plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd77811",
   "metadata": {},
   "source": [
    "Now lets compare to the truth and look at how data assimilation reduced predictive uncertainty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_obs = obs.loc[obs.oname==oname].copy()\n",
    "_obs[\"i\"] = _obs[\"i\"].astype(int)\n",
    "_obs.sort_values(\"i\", inplace=True)\n",
    "obsnmes = _obs.obsnme.tolist()\n",
    "\n",
    "\n",
    "fig,axs = plt.subplots(2,2,figsize=(12,12))\n",
    "\n",
    "\n",
    "ax = axs[0,0]\n",
    "ax.set_aspect(\"equal\")\n",
    "pm = flopy.plot.PlotMapView(model=gwf, ax=ax)\n",
    "#pm.plot_grid(alpha=0.1,lw=0.1)\n",
    "\n",
    "arr = oe.loc[:,obsnmes].mean()\n",
    "\n",
    "\n",
    "vmax = max(oe.loc[:,obsnmes].values.max(), abs(oe.loc[:,obsnmes].values.min()))\n",
    "vmin = oe.loc[:,obsnmes].values.min()\n",
    "vmax = np.ceil(vmax)\n",
    "vmin = 16 \n",
    "pa = pm.plot_array(arr, cmap=\"Reds\", vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(pa, ax=ax, shrink=0.5)\n",
    "\n",
    "ax.set_title('dsi(mean)')\n",
    "\n",
    "\n",
    "ax = axs[0,1]\n",
    "ax.set_aspect(\"equal\")\n",
    "pm = flopy.plot.PlotMapView(model=gwf, ax=ax)\n",
    "#pm.plot_grid(alpha=0.1,lw=0.1)\n",
    "\n",
    "arr = oe.loc[:,obsnmes].std()\n",
    "\n",
    "\n",
    "vmax = max(oe.loc[:,obsnmes].values.max(), abs(oe.loc[:,obsnmes].values.min()))\n",
    "vmin = oe.loc[:,obsnmes].values.min()\n",
    "vmax = np.ceil(vmax)\n",
    "vmin = 16 \n",
    "pa = pm.plot_array(arr, cmap=\"Reds\",)# vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(pa, ax=ax, shrink=0.5)\n",
    "\n",
    "\n",
    "cell = int(obs.loc[target_col].i)\n",
    "arr = arr.values\n",
    "arr[:] = np.nan\n",
    "arr[cell] = 1.0\n",
    "pa = pm.plot_array(arr,)# vmin=vmin, vmax=vmax)\n",
    "\n",
    "ax.set_title('dsi(std)')\n",
    "\n",
    "\n",
    "ax = axs[1,0]\n",
    "ax.set_aspect(\"equal\")\n",
    "pm = flopy.plot.PlotMapView(model=gwf, ax=ax)\n",
    "#pm.plot_grid(alpha=0.1,lw=0.1)\n",
    "\n",
    "arr = oe.loc[:,obsnmes].mean()\n",
    "arr = arr - obs.loc[obsnmes,\"obsval\"].values\n",
    "\n",
    "vmax = max(arr.max(), abs(arr.min()))\n",
    "\n",
    "pa = pm.plot_array(arr, cmap=\"RdBu\", vmin=-vmax,vmax=vmax)\n",
    "plt.colorbar(pa, ax=ax, shrink=0.5)\n",
    "\n",
    "cell = int(obs.loc[target_col].i)\n",
    "arr = arr.values\n",
    "arr[:] = np.nan\n",
    "arr[cell] = 1.0\n",
    "pa = pm.plot_array(arr,)# vmin=vmin, vmax=vmax)\n",
    "\n",
    "ax.set_title('dsi(mean) - truth')\n",
    "\n",
    "\n",
    "ax = axs[1,1]\n",
    "ax.set_aspect(\"equal\")\n",
    "pm = flopy.plot.PlotMapView(model=gwf, ax=ax)\n",
    "\n",
    "arr =  (data.loc[:,obsnmes].std() - oe.loc[:,obsnmes].std())/data.loc[:,obsnmes].std()\n",
    "arr[abs(data.loc[:,obsnmes].std())<1e-3] = 0\n",
    "\n",
    "pa = pm.plot_array(arr, cmap=\"RdBu_r\",vmax=1,vmin=-1)#,vmin=-vmax,vmax=vmax )\n",
    "plt.colorbar(pa, ax=ax, shrink=0.5)\n",
    "\n",
    "ax.set_title('rel std unc reduction [(prior-post)/(prior)]')\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "fig.tight_layout();\n",
    "plt.show()\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obsnmes = obs.loc[obs.oname==\"temp\"].obsnme.tolist()\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "\n",
    "data.loc[:,target_col].hist(ax=ax, alpha=0.5,color='fuchsia',zorder=0)\n",
    "\n",
    "obsen.loc[0].loc[:,target_col].hist(ax=ax,color='0.5')\n",
    "oe.loc[:,target_col].hist(ax=ax,alpha=0.5,color='b')\n",
    "\n",
    "ax.axvline(obs.loc[target_col].obsval, color='r')\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53679f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
